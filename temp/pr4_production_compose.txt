# ============================================================================
# PR-4: Production Compose & Health Checks
# Files: Production docker-compose overlay + health checks
# ============================================================================

# FILE: docker-compose.prod.yml
version: '3.8'

services:
  api:
    environment:
      - ENVIRONMENT=production
      - CORS_ORIGINS=${CORS_ORIGINS:-https://your-domain.com}
      - API_KEY=${API_KEY}
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql+asyncpg://postgres:${POSTGRES_PASSWORD}@postgres:5432/docfoundry
      - RATE_LIMIT_QUERY=30/minute
      - RATE_LIMIT_INGEST=5/minute
      - IVFFLAT_PROBES=10
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  worker:
    environment:
      - ENVIRONMENT=production
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql+asyncpg://postgres:${POSTGRES_PASSWORD}@postgres:5432/docfoundry
      - WORKER_METRICS_PORT=9108
      - CRAWLER_USER_AGENT=DocFoundry/1.0
      - CRAWLER_DEFAULT_DELAY=1.0
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9108/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  postgres:
    environment:
      - POSTGRES_DB=docfoundry
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./ops/postgres/postgresql.conf:/etc/postgresql/postgresql.conf
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d docfoundry"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes --maxmemory 1gb --maxmemory-policy allkeys-lru
    environment:
      - REDIS_REPLICATION_MODE=master
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

  prometheus:
    image: prom/prometheus:v2.45.0
    container_name: prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.wal-compression'
    volumes:
      - ./ops/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    depends_on:
      - api
      - worker

  alertmanager:
    image: prom/alertmanager:v0.25.0
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./ops/alertmanager:/etc/alertmanager
      - alertmanager_data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    depends_on:
      - prometheus

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.12.0
    container_name: postgres-exporter
    environment:
      - DATA_SOURCE_NAME=postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/docfoundry?sslmode=disable
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
    restart: unless-stopped
    depends_on:
      - postgres

  redis-exporter:
    image: oliver006/redis_exporter:v1.51.0
    container_name: redis-exporter
    environment:
      - REDIS_ADDR=redis://redis:6379
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.25'
    restart: unless-stopped
    depends_on:
      - redis

  nginx:
    image: nginx:alpine
    container_name: nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./ops/nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./ops/nginx/ssl:/etc/nginx/ssl
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    depends_on:
      - api

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  alertmanager_data:
    driver: local

networks:
  default:
    driver: bridge

# ============================================================================
# FILE: ops/postgres/postgresql.conf
# PostgreSQL configuration optimized for DocFoundry

# Connection settings
max_connections = 100
shared_buffers = 2GB
effective_cache_size = 6GB
work_mem = 256MB
maintenance_work_mem = 512MB

# WAL settings
wal_buffers = 64MB
checkpoint_completion_target = 0.9
checkpoint_timeout = 10min
max_wal_size = 4GB
min_wal_size = 1GB

# Query planner
random_page_cost = 1.1  # SSD optimized
effective_io_concurrency = 200

# Vector extension settings
shared_preload_libraries = 'vector'

# Logging
log_min_duration_statement = 1000  # Log queries > 1s
log_statement = 'none'
log_duration = off
log_lock_waits = on

# Background writer
bgwriter_delay = 200ms
bgwriter_lru_maxpages = 100
bgwriter_lru_multiplier = 2.0

# Autovacuum (important for vector indexes)
autovacuum = on
autovacuum_max_workers = 3
autovacuum_naptime = 20s

# ============================================================================
# FILE: ops/nginx/nginx.conf
# Nginx configuration for DocFoundry

events {
    worker_connections 1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;
    
    # Security headers
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
    
    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_comp_level 6;
    gzip_types text/plain text/css application/json application/javascript text/xml application/xml;
    
    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=100r/m;
    limit_req_zone $binary_remote_addr zone=upload:10m rate=10r/m;
    
    upstream api_backend {
        least_conn;
        server api:8080 max_fails=3 fail_timeout=30s;
        keepalive 32;
    }
    
    server {
        listen 80;
        server_name _;
        
        # Health check endpoint
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
        
        # API proxy with rate limiting
        location /api/ {
            limit_req zone=api burst=20 nodelay;
            
            proxy_pass http://api_backend/;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Timeouts
            proxy_connect_timeout 30s;
            proxy_send_timeout 30s;
            proxy_read_timeout 30s;
            
            # Buffer settings
            proxy_buffering on;
            proxy_buffer_size 4k;
            proxy_buffers 8 4k;
        }
        
        # Upload endpoint with stricter rate limiting
        location /api/upload {
            limit_req zone=upload burst=5 nodelay;
            client_max_body_size 50M;
            
            proxy_pass http://api_backend/upload;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Extended timeouts for file uploads
            proxy_connect_timeout 60s;
            proxy_send_timeout 300s;
            proxy_read_timeout 300s;
        }
        
        # Metrics (restrict access)
        location /metrics {
            allow 127.0.0.1;
            allow 10.0.0.0/8;
            allow 172.16.0.0/12;
            allow 192.168.0.0/16;
            deny all;
            
            proxy_pass http://api_backend/metrics;
        }
        
        # Static files (if any)
        location /static/ {
            alias /var/www/static/;
            expires 1y;
            add_header Cache-Control "public, immutable";
        }
    }
}

# ============================================================================
# FILE: ops/health/health_checks.py
# Comprehensive health check implementation

import asyncio
import aiohttp
import asyncpg
import redis.asyncio as redis
from typing import Dict, Any, List
import time
import logging
from dataclasses import dataclass

@dataclass
class HealthStatus:
    service: str
    status: str  # "healthy", "degraded", "unhealthy"
    latency_ms: float
    details: Dict[str, Any]

class HealthChecker:
    """Comprehensive health checker for DocFoundry services"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    async def check_database(self, db_url: str) -> HealthStatus:
        """Check PostgreSQL database health"""
        start_time = time.time()
        
        try:
            conn = await asyncpg.connect(db_url)
            
            # Test basic connectivity
            await conn.execute("SELECT 1")
            
            # Check pgvector extension
            vector_result = await conn.fetchval(
                "SELECT EXISTS(SELECT 1 FROM pg_extension WHERE extname = 'vector')"
            )
            
            # Check table existence
            tables_result = await conn.fetch(
                "SELECT tablename FROM pg_tables WHERE schemaname = 'public'"
            )
            
            # Check index health
            index_result = await conn.fetch(
                """
                SELECT schemaname, tablename, indexname, indexdef 
                FROM pg_indexes 
                WHERE schemaname = 'public' AND indexname LIKE '%embedding%'
                """
            )
            
            await conn.close()
            
            latency = (time.time() - start_time) * 1000
            
            details = {
                "pgvector_enabled": vector_result,
                "tables_count": len(tables_result),
                "vector_indexes_count": len(index_result),
                "connection_latency_ms": round(latency, 2)
            }
            
            status = "healthy" if vector_result and len(tables_result) > 0 else "degraded"
            
            return HealthStatus("database", status, latency, details)
            
        except Exception as e:
            latency = (time.time() - start_time) * 1000
            return HealthStatus(
                "database", 
                "unhealthy", 
                latency, 
                {"error": str(e)}
            )
    
    async def check_redis(self, redis_url: str) -> HealthStatus:
        """Check Redis health"""
        start_time = time.time()
        
        try:
            redis_client = redis.from_url(redis_url)
            
            # Test basic connectivity
            await redis_client.ping()
            
            # Check memory usage
            info = await redis_client.info()
            memory_usage = info.get('used_memory', 0)
            max_memory = info.get('maxmemory', 0)
            
            # Test set/get operation
            test_key = "health_check_test"
            await redis_client.set(test_key, "test_value", ex=10)
            test_result = await redis_client.get(test_key)
            await redis_client.delete(test_key)
            
            await redis_client.close()
            
            latency = (time.time() - start_time) * 1000
            
            memory_usage_pct = (memory_usage / max_memory * 100) if max_memory > 0 else 0
            
            details = {
                "memory_used_bytes": memory_usage,
                "memory_used_percent": round(memory_usage_pct, 2),
                "connected_clients": info.get('connected_clients', 0),
                "test_operation": "success" if test_result == b"test_value" else "failed"
            }
            
            # Determine status based on memory usage
            if memory_usage_pct > 90:
                status = "degraded"
            elif test_result != b"test_value":
                status = "degraded"
            else:
                status = "healthy"
            
            return HealthStatus("redis", status, latency, details)
            
        except Exception as e:
            latency = (time.time() - start_time) * 1000
            return HealthStatus(
                "redis", 
                "unhealthy", 
                latency, 
                {"error": str(e)}
            )
    
    async def check_api_endpoint(self, url: str) -> HealthStatus:
        """Check API endpoint health"""
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{url}/healthz", timeout=aiohttp.ClientTimeout(total=10)) as response:
                    latency = (time.time() - start_time) * 1000
                    
                    if response.status == 200:
                        data = await response.json()
                        return HealthStatus(
                            "api", 
                            "healthy", 
                            latency, 
                            {"response": data, "status_code": response.status}
                        )
                    else:
                        return HealthStatus(
                            "api", 
                            "degraded", 
                            latency, 
                            {"status_code": response.status}
                        )
                        
        except Exception as e:
            latency = (time.time() - start_time) * 1000
            return HealthStatus(
                "api", 
                "unhealthy", 
                latency, 
                {"error": str(e)}
            )
    
    async def check_worker_metrics(self, url: str) -> HealthStatus:
        """Check worker metrics endpoint"""
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{url}/", timeout=aiohttp.ClientTimeout(total=10)) as response:
                    latency = (time.time() - start_time) * 1000
                    
                    if response.status == 200:
                        # Could parse metrics here for more detailed health info
                        return HealthStatus(
                            "worker", 
                            "healthy", 
                            latency, 
                            {"metrics_endpoint": "accessible"}
                        )
                    else:
                        return HealthStatus(
                            "worker", 
                            "degraded", 
                            latency, 
                            {"status_code": response.status}
                        )
                        
        except Exception as e:
            latency = (time.time() - start_time) * 1000
            return HealthStatus(
                "worker", 
                "unhealthy", 
                latency, 
                {"error": str(e)}
            )
    
    async def comprehensive_health_check(self, config: Dict[str, str]) -> Dict[str, Any]:
        """Run comprehensive health check across all services"""
        
        tasks = []
        
        if "database_url" in config:
            tasks.append(self.check_database(config["database_url"]))
        
        if "redis_url" in config:
            tasks.append(self.check_redis(config["redis_url"]))
        
        if "api_url" in config:
            tasks.append(self.check_api_endpoint(config["api_url"]))
        
        if "worker_url" in config:
            tasks.append(self.check_worker_metrics(config["worker_url"]))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        health_checks = []
        overall_status = "healthy"
        
        for result in results:
            if isinstance(result, HealthStatus):
                health_checks.append({
                    "service": result.service,
                    "status": result.status,
                    "latency_ms": result.latency_ms,
                    "details": result.details
                })
                
                # Determine overall status
                if result.status == "unhealthy":
                    overall_status = "unhealthy"
                elif result.status == "degraded" and overall_status != "unhealthy":
                    overall_status = "degraded"
            else:
                # Handle exceptions
                health_checks.append({
                    "service": "unknown",
                    "status": "unhealthy",
                    "latency_ms": 0,
                    "details": {"error": str(result)}
                })
                overall_status = "unhealthy"
        
        return {
            "overall_status": overall_status,
            "timestamp": time.time(),
            "checks": health_checks,
            "summary": {
                "total_checks": len(health_checks),
                "healthy": len([c for c in health_checks if c["status"] == "healthy"]),
                "degraded": len([c for c in health_checks if c["status"] == "degraded"]),
                "unhealthy": len([c for c in health_checks if c["status"] == "unhealthy"])
            }
        }

# Usage example
async def main():
    checker = HealthChecker()
    config = {
        "database_url": "postgresql://postgres:password@postgres:5432/docfoundry",
        "redis_url": "redis://redis:6379",
        "api_url": "http://api:8080",
        "worker_url": "http://worker:9108"
    }
    
    result = await checker.comprehensive_health_check(config)
    print(result)

# ============================================================================
# FILE: scripts/production-setup.sh
#!/bin/bash

# Production setup script for DocFoundry

set -e

echo "🚀 Starting DocFoundry production setup..."

# Check required environment variables
required_vars=("POSTGRES_PASSWORD" "API_KEY" "CORS_ORIGINS")
for var in "${required_vars[@]}"; do
    if [ -z "${!var}" ]; then
        echo "❌ Error: Environment variable $var is not set"
        exit 1
    fi
done

echo "✅ Environment variables validated"

# Create necessary directories
mkdir -p ops/prometheus/rules
mkdir -p ops/alertmanager
mkdir -p ops/nginx/ssl
mkdir -p logs

echo "✅ Directories created"

# Generate SSL certificates (self-signed for development)
if [ ! -f ops/nginx/ssl/cert.pem ]; then
    echo "🔐 Generating self-signed SSL certificates..."
    openssl req -x509 -newkey rsa:4096 -keyout ops/nginx/ssl/key.pem -out ops/nginx/ssl/cert.pem -days 365 -nodes -subj "/C=US/ST=State/L=City/O=Organization/CN=localhost"
fi

# Build and start services
echo "🏗️ Building DocFoundry services..."
docker-compose -f docker-compose.yml -f docker-compose.prod.yml build

echo "🔄 Starting services..."
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d

# Wait for database to be ready
echo "⏳ Waiting for database to be ready..."
max_attempts=30
attempt=1
while [ $attempt -le $max_attempts ]; do
    if docker-compose -f docker-compose.yml -f docker-compose.prod.yml exec -T postgres pg_isready -U postgres -d docfoundry; then
        echo "✅ Database is ready"
        break
    fi
    echo "Attempt $attempt/$max_attempts: Database not ready, waiting..."
    sleep 2
    attempt=$((attempt + 1))
done

if [ $attempt -gt $max_attempts ]; then
    echo "❌ Database failed to start within expected time"
    exit 1
fi

# Run database migrations
echo "🔧 Running database migrations..."
docker-compose -f docker-compose.yml -f docker-compose.prod.yml exec -T api alembic upgrade head

# Optimize vector indexes
echo "🎯 Optimizing vector indexes..."
docker-compose -f docker-compose.yml -f docker-compose.prod.yml exec -T api python -c "
import asyncio
from services.shared.db import optimize_all_indexes
asyncio.run(optimize_all_indexes())
"

# Health check
echo "🏥 Running health checks..."
sleep 10  # Give services time to start

health_check() {
    local service=$1
    local url=$2
    local expected_status=${3:-200}
    
    if curl -f -s -o /dev/null -w "%{http_code}" "$url" | grep -q "$expected_status"; then
        echo "✅ $service health check passed"
        return 0
    else
        echo "❌ $service health check failed"
        return 1
    fi
}

# Run health checks
health_check "API" "http://localhost:8080/healthz"
health_check "Prometheus" "http://localhost:9090/-/healthy"
health_check "Alertmanager" "http://localhost:9093/-/healthy"

echo "🎉 DocFoundry production setup completed successfully!"
echo ""
echo "📊 Access points:"
echo "  • API: http://localhost:8080"
echo "  • Prometheus: http://localhost:9090"
echo "  • Alertmanager: http://localhost:9093"
echo ""
echo "📝 Next steps:"
echo "  1. Configure your Slack webhook in ops/alertmanager/alertmanager.yml"
echo "  2. Update CORS_ORIGINS for your frontend domain"
echo "  3. Set up proper SSL certificates for production"
echo "  4. Configure backup strategy for PostgreSQL and Redis"
echo ""
echo "🔍 Monitor logs with:"
echo "  docker-compose -f docker-compose.yml -f docker-compose.prod.yml logs -f"

# ============================================================================
# FILE: scripts/health-check.sh
#!/bin/bash

# Simple health check script that can be run from monitoring systems

set -e

API_URL=${API_URL:-"http://localhost:8080"}
TIMEOUT=${TIMEOUT:-10}

echo "🏥 Running DocFoundry health check..."

# Function to check service health
check_service() {
    local name=$1
    local url=$2
    local timeout=${3:-$TIMEOUT}
    
    echo -n "Checking $name... "
    
    if timeout $timeout curl -f -s "$url" > /dev/null 2>&1; then
        echo "✅ OK"
        return 0
    else
        echo "❌ FAILED"
        return 1
    fi
}

# Check all services
failed_checks=0

check_service "API Health" "$API_URL/healthz" || ((failed_checks++))
check_service "API Metrics" "$API_URL/metrics" || ((failed_checks++))
check_service "Prometheus" "http://localhost:9090/-/healthy" || ((failed_checks++))
check_service "Alertmanager" "http://localhost:9093/-/healthy" || ((failed_checks++))

# Summary
echo ""
if [ $failed_checks -eq 0 ]; then
    echo "🎉 All health checks passed!"
    exit 0
else
    echo "❌ $failed_checks health check(s) failed"
    exit 1
fi
            